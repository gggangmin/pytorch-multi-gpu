{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://tutorials.pytorch.kr/beginner/former_torchies/parallelism_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataParallel Moduel 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parallel \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DataParallelModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Linear(10, 20)\n",
    "\n",
    "        # wrap block2 in DataParallel\n",
    "        self.block2 = nn.Linear(20, 20)\n",
    "        self.block2 = nn.DataParallel(self.block2)\n",
    "\n",
    "        self.block3 = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naiive 하게 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parallel(module, input, device_ids, output_device=None):\n",
    "    if not device_ids:\n",
    "        return module(input)\n",
    "\n",
    "    if output_device is None:\n",
    "        output_device = device_ids[0]\n",
    "\n",
    "    replicas = nn.parallel.replicate(module, device_ids) #model 복제\n",
    "    inputs = nn.parallel.scatter(input, device_ids) #input 분산\n",
    "    replicas = replicas[:len(inputs)] \n",
    "    outputs = nn.parallel.parallel_apply(replicas, inputs) #분산된 input을 분산된 모델에 적용\n",
    "    return nn.parallel.gather(outputs, output_device) #분산된 input 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "[Epoch:    1] cost = 0.316413641\n",
      "[Epoch:    2] cost = 0.117265143\n",
      "[Epoch:    3] cost = 0.0889329165\n",
      "Test Accuracy:  98.45 %\n"
     ]
    }
   ],
   "source": [
    "# mnist 예제\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transfroms\n",
    " \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "print(device + \" is available\")\n",
    " \n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    " \n",
    "# MNIST 데이터셋 로드\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
    "    ])\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
    "    ])\n",
    ")\n",
    " \n",
    "# train_loader, test_loader 생성\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    " \n",
    "# input size를 알기 위해서\n",
    "examples = enumerate(train_set)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    " \n",
    "class ConvNet(nn.Module):\n",
    "  def __init__(self): # layer 정의\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # input size = 28x28 \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        # ((W-K+2P)/S)+1 공식으로 인해 ((28-5+0)/1)+1=24 -> 24x24로 변환\n",
    "        # maxpooling하면 12x12\n",
    "  \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        # ((12-5+0)/1)+1=8 -> 8x8로 변환\n",
    "        # maxpooling하면 4x4\n",
    "\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False) # 랜덤하게 뉴런을 종료해서 학습을 방해해 학습이 학습용 데이터에 치우치는 현상을 막기 위해 사용\n",
    "        self.mp = nn.MaxPool2d(2)  # 오버피팅을 방지하고, 연산에 들어가는 자원을 줄이기 위해 maxpolling\n",
    "        self.fc1 = nn.Linear(320,100) # 4x4x20 vector로 flat한 것을 100개의 출력으로 변경\n",
    "        self.fc2 = nn.Linear(100,10) # 100개의 출력을 10개의 출력으로 변경\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = F.relu(self.mp(self.conv1(x))) # convolution layer 1번에 relu를 씌우고 maxpool, 결과값은 12x12x10\n",
    "        x = F.relu(self.mp(self.conv2(x))) # convolution layer 2번에 relu를 씌우고 maxpool, 결과값은 4x4x20\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.fc1(x) # fc1 레이어에 삽입\n",
    "        x = self.fc2(x) # fc2 레이어에 삽입\n",
    "        return F.log_softmax(x) # fully-connected layer에 넣고 logsoftmax 적용\n",
    " \n",
    "model = ConvNet().to(device) # CNN instance 생성\n",
    "# Cost Function과 Optimizer 선택\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    " \n",
    "for epoch in range(epochs): # epochs수만큼 반복\n",
    "    avg_cost = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() # 모든 model의 gradient 값을 0으로 설정\n",
    "        hypothesis = model(data) # 모델을 forward pass해 결과값 저장 \n",
    "        cost = criterion(hypothesis, target) # output과 target의 loss 계산\n",
    "        cost.backward() # backward 함수를 호출해 gradient 계산\n",
    "        optimizer.step() # 모델의 학습 파라미터 갱신\n",
    "        avg_cost += cost / len(train_loader) # loss 값을 변수에 누적하고 train_loader의 개수로 나눔 = 평균\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    " \n",
    "# test\n",
    "model.eval() # evaluate mode로 전환 dropout 이나 batch_normalization 해제 \n",
    "with torch.no_grad(): # grad 해제 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(data)\n",
    "        preds = torch.max(out.data, 1)[1] # 출력이 분류 각각에 대한 값으로 나타나기 때문에, 가장 높은 값을 갖는 인덱스를 추출\n",
    "        total += len(target) # 전체 클래스 개수 \n",
    "        correct += (preds==target).sum().item() # 예측값과 실제값이 같은지 비교\n",
    "        \n",
    "    print('Test Accuracy: ', 100.*correct/total, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist for parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0  is available\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/proj01/anaconda3/envs/t/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# mnist 예제\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transfroms\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(777)\n",
    "torch.cuda.manual_seed_all(777)\n",
    "print(device, \" is available\")\n",
    " \n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    " \n",
    "# MNIST 데이터셋 로드\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
    "    ])\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
    "    ])\n",
    ")\n",
    " \n",
    "# train_loader, test_loader 생성\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    " \n",
    "# input size를 알기 위해서\n",
    "examples = enumerate(train_set)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    " \n",
    "class ConvNet(nn.Module):\n",
    "  def __init__(self): # layer 정의\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # input size = 28x28 \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        # ((W-K+2P)/S)+1 공식으로 인해 ((28-5+0)/1)+1=24 -> 24x24로 변환\n",
    "        # maxpooling하면 12x12\n",
    "  \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        # ((12-5+0)/1)+1=8 -> 8x8로 변환\n",
    "        # maxpooling하면 4x4\n",
    "\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False) # 랜덤하게 뉴런을 종료해서 학습을 방해해 학습이 학습용 데이터에 치우치는 현상을 막기 위해 사용\n",
    "        self.mp = nn.MaxPool2d(2)  # 오버피팅을 방지하고, 연산에 들어가는 자원을 줄이기 위해 maxpolling\n",
    "        self.fc1 = nn.Linear(320,100) # 4x4x20 vector로 flat한 것을 100개의 출력으로 변경\n",
    "        self.fc2 = nn.Linear(100,10) # 100개의 출력을 10개의 출력으로 변경\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = F.relu(self.mp(self.conv1(x))) # convolution layer 1번에 relu를 씌우고 maxpool, 결과값은 12x12x10\n",
    "        x = F.relu(self.mp(self.conv2(x))) # convolution layer 2번에 relu를 씌우고 maxpool, 결과값은 4x4x20\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.fc1(x) # fc1 레이어에 삽입\n",
    "        x = self.fc2(x) # fc2 레이어에 삽입\n",
    "        return F.log_softmax(x) # fully-connected layer에 넣고 logsoftmax 적용\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = ConvNet().to(device) # CNN instance 생성\n",
    "model = ConvNet()\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# Cost Function과 Optimizer 선택\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    " \n",
    "for epoch in range(epochs): # epochs수만큼 반복\n",
    "    avg_cost = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() # 모든 model의 gradient 값을 0으로 설정\n",
    "        hypothesis = model(data) # 모델을 forward pass해 결과값 저장 \n",
    "        cost = criterion(hypothesis, target) # output과 target의 loss 계산\n",
    "        cost.backward() # backward 함수를 호출해 gradient 계산\n",
    "        optimizer.step() # 모델의 학습 파라미터 갱신\n",
    "        avg_cost += cost / len(train_loader) # loss 값을 변수에 누적하고 train_loader의 개수로 나눔 = 평균\n",
    "        \n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    "'''\n",
    "# test\n",
    "model.eval() # evaluate mode로 전환 dropout 이나 batch_normalization 해제 \n",
    "with torch.no_grad(): # grad 해제 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(data)\n",
    "        preds = torch.max(out.data, 1)[1] # 출력이 분류 각각에 대한 값으로 나타나기 때문에, 가장 높은 값을 갖는 인덱스를 추출\n",
    "        total += len(target) # 전체 클래스 개수 \n",
    "        correct += (preds==target).sum().item() # 예측값과 실제값이 같은지 비교\n",
    "        \n",
    "    print('Test Accuracy: ', 100.*correct/total, '%')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일부는 cpu 일부는 gpu에서 연산할때 사용 예제\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class DistributedModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            embedding=nn.Embedding(1000, 10),\n",
    "            rnn=nn.Linear(10, 10).to(device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CPU에서 연산합니다.\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # GPU로 보냅니다.\n",
    "        x = x.to(device)``\n",
    "\n",
    "        # GPU에서 연산합니다.\n",
    "        x = self.rnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
